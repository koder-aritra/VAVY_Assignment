{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "SlNw67oTRrVY",
        "outputId": "fba2069c-df33-4a7c-911d-c439f273aa7d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3969b1f7-2ee2-49fa-8115-b897552dc71d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3969b1f7-2ee2-49fa-8115-b897552dc71d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"azazel0201\",\"key\":\"258a2565f9959f5e026e5237fe107179\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "pCvQGHXCSPbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d marcosgabriel/photovoltaic-system-thermography"
      ],
      "metadata": {
        "id": "p7N_wh-xSX6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95a89fd-e07c-473a-904a-ee3c07b9c722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading photovoltaic-system-thermography.zip to /content\n",
            " 84% 73.0M/86.6M [00:01<00:00, 62.5MB/s]\n",
            "100% 86.6M/86.6M [00:01<00:00, 75.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/photovoltaic-system-thermography.zip"
      ],
      "metadata": {
        "id": "GTAB9Ll7Ss5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AlexeyAB/darknet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "befqZEbPdaNX",
        "outputId": "b9182e54-c110-43a9-96c2-872bc40f3109"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'darknet'...\n",
            "remote: Enumerating objects: 15833, done.\u001b[K\n",
            "remote: Total 15833 (delta 0), reused 0 (delta 0), pack-reused 15833\u001b[K\n",
            "Receiving objects: 100% (15833/15833), 14.39 MiB | 15.41 MiB/s, done.\n",
            "Resolving deltas: 100% (10666/10666), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd darknet\n",
        "!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n",
        "!sed -i 's/GPU=0/GPU=1/' Makefile\n",
        "!make\n"
      ],
      "metadata": {
        "id": "VJwmHa9pdq5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xWshEh-fSTI",
        "outputId": "6e14ef21-615f-4666-f389-c1b3649d4e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-04 19:03:38--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  18.1MB/s    in 8.6s    \n",
            "\n",
            "2024-04-04 19:03:47 (27.6 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./darknet detect cfg/yolov3.cfg yolov3.weights /content/dataset_1/images/001R.jpg"
      ],
      "metadata": {
        "id": "KhnQvpWxfsDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def parse_json_annotations(json_dir, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]\n",
        "\n",
        "    for json_file in json_files:\n",
        "        with open(os.path.join(json_dir, json_file), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        img_name = os.path.splitext(json_file)[0] + '.txt'\n",
        "        with open(os.path.join(output_dir, img_name), 'w') as f:\n",
        "            for instance in data['instances']:\n",
        "                x_center = instance['center']['x'] / IMAGE_WIDTH\n",
        "                y_center = instance['center']['y'] / IMAGE_HEIGHT\n",
        "                width = (instance['corners'][1]['x'] - instance['corners'][0]['x']) / IMAGE_WIDTH\n",
        "                height = (instance['corners'][2]['y'] - instance['corners'][0]['y']) / IMAGE_HEIGHT\n",
        "                class_id = 0  # Assuming only one class for now\n",
        "                if instance['defected_module']:\n",
        "                    class_id = 1  # Defected module class\n",
        "                f.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "# Define the width and height of your images\n",
        "IMAGE_WIDTH = 640\n",
        "IMAGE_HEIGHT = 480\n",
        "\n",
        "# Directory containing JSON annotations\n",
        "json_dir = '/content/dataset_1/annotations/'\n",
        "\n",
        "# Directory to save YOLO-formatted annotation files\n",
        "output_dir = '/content/yolo_annotations/'\n",
        "\n",
        "parse_json_annotations(json_dir, output_dir)\n"
      ],
      "metadata": {
        "id": "aFSrj9CyglxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "./darknet detector train data/obj.data cfg/yolov3.cfg darknet53.conv.74 -map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ckbh3GyShXAc",
        "outputId": "d7f45726-1cc1-4976-e989-a27303c2dd3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-7912a0b241ec>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7912a0b241ec>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ./darknet detector train data/obj.data cfg/yolov3.cfg darknet53.conv.74 -map\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageOps\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, txt_file, transform=None, target_size=416, padding_value=[0, 0, 0, 0, 2]):\n",
        "        self.data = []\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "        self.padding_value = padding_value\n",
        "\n",
        "        with open(txt_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                line = line.strip().split()\n",
        "                image_path = line[0]\n",
        "                bboxes = [list(map(float, line[i:i+4])) for i in range(1, len(line), 5)]\n",
        "                self.data.append((image_path, bboxes))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, bboxes = self.data[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Resize image maintaining aspect ratio\n",
        "        width, height = image.size\n",
        "        aspect_ratio = width / height\n",
        "        if aspect_ratio > 1:\n",
        "            new_width = self.target_size\n",
        "            new_height = int(self.target_size / aspect_ratio)\n",
        "        else:\n",
        "            new_height = self.target_size\n",
        "            new_width = int(self.target_size * aspect_ratio)\n",
        "\n",
        "        image = image.resize((new_width, new_height))\n",
        "\n",
        "        # Pad image to target size if necessary\n",
        "        pad_width = max(0, self.target_size - new_width)\n",
        "        pad_height = max(0, self.target_size - new_height)\n",
        "        if pad_width > 0 or pad_height > 0:  # Pad if either pad_width or pad_height is greater than 0\n",
        "            pad_color = tuple(self.padding_value[:3])\n",
        "            padding = Image.new(image.mode, (self.target_size, self.target_size), pad_color)\n",
        "            padding.paste(image, ((self.target_size - new_width) // 2, (self.target_size - new_height) // 2))\n",
        "            image = padding\n",
        "\n",
        "        # Convert image to tensor\n",
        "        image = transforms.ToTensor()(image)\n",
        "\n",
        "        # Apply custom transformation to mask padded regions\n",
        "        mask = torch.all(image == torch.tensor(self.padding_value[:3]).view(3, 1, 1), dim=0)\n",
        "        mask = mask.float()\n",
        "        image = image * (1 - mask)\n",
        "\n",
        "        # Apply normalization\n",
        "        image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
        "\n",
        "        return image, bboxes\n",
        "\n",
        "\n",
        "class MaskPaddedRegions(object):\n",
        "    def __init__(self, padding_value):\n",
        "        self.padding_value = padding_value\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, label = sample['image'], sample['label']\n",
        "        # Calculate mask to identify padded regions\n",
        "        mask = torch.all(image == self.padding_value, dim=0)\n",
        "        # Convert mask to dtype=torch.float32 for multiplication\n",
        "        mask = mask.float()\n",
        "        # Apply mask to the image to zero out padded regions\n",
        "        image = image * (1 - mask)\n",
        "        return {'image': image, 'label': label}"
      ],
      "metadata": {
        "id": "wZf7lJBcJ7l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "dataset_folder = 'dataset_1'\n",
        "images_folder = os.path.join(dataset_folder, 'images')\n",
        "annotations_folder = os.path.join(dataset_folder, 'annotations')\n",
        "output_file = 'dataset.txt'  # File to save dataset information\n",
        "\n",
        "# Define your classes\n",
        "classes = ['defected', 'not defected']\n",
        "\n",
        "# Initialize the dataset list\n",
        "dataset = []\n",
        "\n",
        "# Iterate through annotation files\n",
        "for annotation_file in os.listdir(annotations_folder):\n",
        "    if annotation_file.endswith('.json'):\n",
        "        image_name = os.path.splitext(annotation_file)[0]\n",
        "        image_path = os.path.join(images_folder, image_name + '.jpg')  # Assuming images are in JPEG format\n",
        "\n",
        "        # Load JSON file\n",
        "        with open(os.path.join(annotations_folder, annotation_file), 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Extract bounding box annotations for both classes\n",
        "        annotations = []\n",
        "        for instance in data['instances']:\n",
        "            if instance['defected_module']:\n",
        "                class_id = classes.index('defected')\n",
        "            else:\n",
        "                class_id = classes.index('not defected')\n",
        "\n",
        "            corners = instance['corners']\n",
        "            x_min = min(corners, key=lambda x: x['x'])['x']\n",
        "            x_max = max(corners, key=lambda x: x['x'])['x']\n",
        "            y_min = min(corners, key=lambda x: x['y'])['y']\n",
        "            y_max = max(corners, key=lambda x: x['y'])['y']\n",
        "\n",
        "            annotations.append((x_min, y_min, x_max, y_max, class_id))\n",
        "\n",
        "        # Add image path and annotations to the dataset\n",
        "        dataset.append((image_path, annotations))\n",
        "\n",
        "# Write dataset to file\n",
        "with open(output_file, 'w') as f:\n",
        "    for image_path, annotations in dataset:\n",
        "        f.write(image_path)\n",
        "        for annotation in annotations:\n",
        "            f.write(' ')\n",
        "            f.write(' '.join(str(coord) for coord in annotation))\n",
        "        f.write('\\n')\n",
        "\n",
        "print(\"Dataset created successfully.\")\n"
      ],
      "metadata": {
        "id": "8AstK4AxJ4p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the input file for reading\n",
        "with open('dataset.txt', 'r') as infile:\n",
        "    # Read all lines from the file\n",
        "    lines = infile.readlines()\n",
        "\n",
        "max_points = 0\n",
        "for l in lines:\n",
        "    points = (l.split(' ')[1:])\n",
        "    max_points = max(len(points), max_points)\n",
        "\n",
        "\n",
        "for i, l in enumerate(lines):\n",
        "    points = (l.split(' ')[1:])\n",
        "    if len(points) != max_points:\n",
        "        add = (max_points - len(points)) // 5  # Calculate the number of sets of (0 0 0 0 2) to add\n",
        "        padding = ' '.join(['0', '0', '0', '0', '2'] * add)\n",
        "        lines[i] = lines[i].rstrip() + ' ' + padding + '\\n'  # Add padding and newline\n",
        "\n",
        "\n",
        "with open('dataset2.txt', 'w') as outfile:\n",
        "    outfile.writelines(lines)\n",
        "exit()\n",
        "# Determine the maximum number of data points in any row\n",
        "max_data_points = max(len(line.split()) for line in lines)\n",
        "print(max_data_points)\n",
        "\n",
        "# Pad each row with (0 0 0 0 2) where necessary\n",
        "for i, line in enumerate(lines):\n",
        "    data_points = line.split()\n",
        "    num_data_points = len(data_points)\n",
        "    if num_data_points < max_data_points:\n",
        "        # Pad the row with (0 0 0 0 2) until it reaches the maximum length\n",
        "        padding = ['0', '0', '0', '0', '2'] * (max_data_points - num_data_points)\n",
        "        padded_line = ' '.join(data_points + padding)\n",
        "        lines[i] = padded_line + '\\n'  # Add newline character\n",
        "\n",
        "# Write the modified lines back to the file\n",
        "with open('dataset2.txt', 'w') as outfile:\n",
        "    outfile.writelines(lines)\n"
      ],
      "metadata": {
        "id": "6h5duGxwJ_Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Step 1: Prepare Dataset\n",
        "\n",
        "# Load images and corresponding JSON files\n",
        "def load_dataset(image_dir, annotation_dir):\n",
        "    image_paths = []\n",
        "    annotations = []\n",
        "\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "            annotation_path = os.path.join(annotation_dir, filename.replace('.jpg', '.json').replace('.png', '.json'))\n",
        "            if os.path.exists(annotation_path):\n",
        "                image_paths.append(image_path)\n",
        "                with open(annotation_path, 'r') as f:\n",
        "                    annotations.append(json.load(f)['instances'])\n",
        "\n",
        "    return image_paths, annotations\n",
        "\n",
        "# Step 2: Prepare YOLOv3 Model\n",
        "\n",
        "# Define paths to YOLOv3 configuration file, pre-trained weights, and class names\n",
        "yolo_cfg_path = \"yolov3.cfg\"\n",
        "yolo_weights_path = \"yolov3.weights\"\n",
        "class_names_path = \"coco2.names\"\n",
        "\n",
        "# Update YOLOv3 configuration file for rectangle detection\n",
        "def update_yolo_config_for_rectangles(yolo_cfg_path):\n",
        "    with open(yolo_cfg_path, 'r') as f:\n",
        "        cfg = f.read()\n",
        "\n",
        "    # Modify last layer for rectangle detection\n",
        "    cfg = cfg.replace('classes=80', 'classes=1')\n",
        "    cfg = cfg.replace('filters=255', 'filters=30')  # (4+1)*6 = 30, adjust filters accordingly\n",
        "    cfg = cfg.replace('activation=linear', 'activation=linear')  # linear activation for regression\n",
        "\n",
        "    with open(yolo_cfg_path, 'w') as f:\n",
        "        f.write(cfg)\n",
        "\n",
        "# Update YOLOv3 configuration file\n",
        "update_yolo_config_for_rectangles(yolo_cfg_path)\n",
        "\n",
        "# Load YOLOv3 configuration and weights\n",
        "net = cv2.dnn.readNet(yolo_weights_path, yolo_cfg_path)\n",
        "\n",
        "\n",
        "\n",
        "# Load class names\n",
        "classes = None\n",
        "with open(class_names_path, 'r') as f:\n",
        "    classes = f.read().strip().split('\\n')\n",
        "\n",
        "# Step 3: Load Pre-trained Weights (already done in Step 2)\n",
        "\n",
        "# Step 4: Train the Model (fine-tuning)\n",
        "\n",
        "def train_yolo_model(image_paths, annotations):\n",
        "    # Fine-tune YOLOv3 model using images and annotations\n",
        "    for i, image_path in enumerate(image_paths):\n",
        "        image = cv2.imread(image_path)\n",
        "        height, width, channels = image.shape\n",
        "        # print(height, width, channels)\n",
        "        # exit()\n",
        "\n",
        "        # Extract rectangle annotations\n",
        "        rectangles = annotations[i]\n",
        "\n",
        "\n",
        "        # Define target output for each rectangle\n",
        "        targets = []\n",
        "        for rect in rectangles:\n",
        "            corners = rect[\"corners\"]\n",
        "            center = rect[\"center\"]\n",
        "\n",
        "            # print(corners)\n",
        "            # print(center)\n",
        "            x_center = center[\"x\"]\n",
        "            y_center = center[\"y\"]\n",
        "\n",
        "            for corner in corners:\n",
        "                x = corner['x']\n",
        "                y = corner['y']\n",
        "                target_x = (x + x_center) / (2 * width)\n",
        "                target_y = (y + y_center) / (2 * height)\n",
        "                target_width = abs(x - x_center) / width\n",
        "                target_height = abs(y - y_center) / height\n",
        "                targets.append([target_x, target_y, target_width, target_height])\n",
        "\n",
        "        # Create input blob\n",
        "        blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "        print(blob)\n",
        "        cv2.imshow(\"image\", cv2.imread(image_path))\n",
        "        cv2.waitKey(0)\n",
        "        net.setInput(blob)\n",
        "\n",
        "        # Forward pass\n",
        "        outs = net.forward()\n",
        "\n",
        "        print(outs)\n",
        "        exit()\n",
        "        # Compute loss (MSE loss or any suitable regression loss)\n",
        "        loss = compute_loss(outs, targets)\n",
        "        # Update model weights\n",
        "\n",
        "# Step 5: Evaluate Performance (Optional)\n",
        "def compute_loss(predictions, targets):\n",
        "    # Convert predictions and targets to PyTorch tensors\n",
        "    predictions = torch.tensor(predictions, requires_grad=True)\n",
        "    targets = torch.tensor(targets, requires_grad=False)\n",
        "\n",
        "    # Compute Mean Squared Error (MSE) loss\n",
        "    loss = F.mse_loss(predictions, targets)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Get the gradients\n",
        "    gradients = predictions.grad\n",
        "\n",
        "    # Update the model weights (for demonstration purposes)\n",
        "    # Replace this with actual weight update code in your YOLOv3 implementation\n",
        "    with torch.no_grad():\n",
        "        learning_rate = 0.001  # Adjust as needed\n",
        "        predictions -= learning_rate * gradients\n",
        "\n",
        "    return loss.item()  # Return the loss value\n",
        "\n",
        "def detect_rectangles(image_path):\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, channels = image.shape\n",
        "\n",
        "    # Create input blob\n",
        "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "\n",
        "    # Forward pass\n",
        "    outs = net.forward()\n",
        "\n",
        "    # Process output\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > 0.5:\n",
        "                # Object detected\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "                # Rectangle coordinates\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "                # Draw rectangle on image\n",
        "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Example usage:\n",
        "image_paths, annotations = load_dataset(\"dataset_1\\\\images\", \"dataset_1\\\\annotations\")\n",
        "# print(annotations[0])\n",
        "train_yolo_model(image_paths, annotations)\n",
        "# result_image = detect_rectangles(\"test_image.jpg\")\n",
        "# cv2.imshow(\"Result\", result_image)\n",
        "# cv2.waitKey(0)\n",
        "# cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "42GN-bg2jApm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load YOLO\n",
        "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
        "classes = []\n",
        "with open(\"coco.names\", \"r\") as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "layer_names = net.getLayerNames()\n",
        "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "def detect_objects(image):\n",
        "    height, width, channels = image.shape\n",
        "\n",
        "    # Detecting objects\n",
        "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outs = net.forward(output_layers)\n",
        "\n",
        "    # Showing informations on the screen\n",
        "    class_ids = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "            if confidence > 0.5:\n",
        "                # Object detected\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "                # Rectangle coordinates\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        if i in indexes:\n",
        "            x, y, w, h = boxes[i]\n",
        "            label = str(classes[class_ids[i]])\n",
        "            color = (0, 255, 0)\n",
        "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "            cv2.putText(image, label, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
        "\n",
        "    return image, len(boxes)\n",
        "\n",
        "# Read the input image\n",
        "image = cv2.imread(\"dataset_1\\\\images\\\\001R.jpg\")\n",
        "\n",
        "# Detect objects and draw bounding boxes\n",
        "result, num_objects = detect_objects(image.copy())\n",
        "\n",
        "# Display the result\n",
        "cv2.imshow(\"Result\", result)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Print the number of objects detected\n",
        "print(\"Number of objects detected:\", num_objects)\n"
      ],
      "metadata": {
        "id": "4N4zuA1IJu3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def overlay_images(folder_path, output_path, transparency=0.5):\n",
        "    images = []\n",
        "\n",
        "    # Iterate through the images in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            img = cv2.imread(image_path)\n",
        "            images.append(img)\n",
        "\n",
        "    # Create a blank image to overlay on\n",
        "    base_image = images[0].copy()\n",
        "\n",
        "    # Overlay each image on top of the base image\n",
        "    for img in images[1:]:\n",
        "        cv2.addWeighted(img, transparency, base_image, 1 - transparency, 0, base_image)\n",
        "\n",
        "    # Show the final image\n",
        "    cv2.imshow(\"Overlayed Image\", base_image)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "folder_path = \"dataset_1/images\"\n",
        "output_path = \"image.png\"\n",
        "overlay_images(folder_path, output_path, transparency=0.5)\n"
      ],
      "metadata": {
        "id": "1oLBjKQtJtni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def merge_close_coordinates(points, threshold=5):\n",
        "    merged_points = []\n",
        "    i = 0\n",
        "    while i < len(points):\n",
        "        merged_point = points[i].flatten()\n",
        "        x1, y1 = merged_point\n",
        "\n",
        "        j = i + 1\n",
        "        while j < len(points):\n",
        "            next_point = points[j].flatten()\n",
        "            x2, y2 = next_point\n",
        "\n",
        "            # Check if x and y values are close enough\n",
        "            if abs(x2 - x1) < threshold and abs(y2 - y1) < threshold:\n",
        "                # Merge the points\n",
        "                merged_point[0] = (x1 + x2) // 2\n",
        "                merged_point[1] = (y1 + y2) // 2\n",
        "                # Move the index to the next point\n",
        "                i = j\n",
        "            else:\n",
        "                # If points are not close enough, break the loop\n",
        "                break\n",
        "            j += 1\n",
        "\n",
        "        merged_points.append(merged_point)\n",
        "        i += 1\n",
        "\n",
        "    return merged_points\n",
        "\n",
        "def detect_rectangles(image):\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply Gaussian blur to reduce noise\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "    # Detect edges using Canny edge detection\n",
        "    edges = cv2.Canny(blurred, 50, 150)\n",
        "\n",
        "    # Find contours in the edge-detected image\n",
        "    contours, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Initialize a counter for rectangles\n",
        "    rectangle_count = 0\n",
        "\n",
        "    # Iterate through detected contours and find rectangles\n",
        "    for contour in contours:\n",
        "        perimeter = cv2.arcLength(contour, True)\n",
        "        if perimeter > 300:\n",
        "            epsilon = 0.02 * perimeter\n",
        "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "            # Merge close coordinates\n",
        "            merged_points = merge_close_coordinates(approx)\n",
        "            approx = np.array(merged_points).reshape((-1, 1, 2)).astype(np.int32)\n",
        "            # If the polygon has 4 vertices (a rectangle), draw a bounding box\n",
        "            if len(approx) < 20000000:\n",
        "                print(approx)\n",
        "                x, y, w, h = cv2.boundingRect(approx)\n",
        "                cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "                rectangle_count += 1\n",
        "\n",
        "    return image, rectangle_count\n",
        "\n",
        "# Read the input image\n",
        "image = cv2.imread(\"dataset_1\\\\images\\\\001R.jpg\")\n",
        "\n",
        "# Detect rectangles and draw bounding boxes\n",
        "result, num_rectangles = detect_rectangles(image.copy())\n",
        "\n",
        "# Display the result\n",
        "cv2.imshow(\"Result\", result)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Print the number of rectangles detected\n",
        "print(\"Number of rectangles:\", num_rectangles)\n"
      ],
      "metadata": {
        "id": "aivjb0wPJyF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}